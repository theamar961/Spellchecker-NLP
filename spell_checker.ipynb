{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook I will be walking you through the different type of spelling mistakes that occur in a document\n",
    "\n",
    "#For any NLP task, we usually tokenize the words as a preprocessing step. We split the document with spaces.\n",
    "#This is a sample list of tokens which we extracted from a document.\n",
    "#Task is to correct or seperate the tokens\n",
    "list_words=['viruz','Microsoft','handuclean','feeeevvvveerrrr','facemaskk','sanitized/dry','million']\n",
    "corrected=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries which we will be using for our task\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "#!pip install wordninja\n",
    "import wordninja\n",
    "#!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "import spacy \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft ORG\n",
      "million CARDINAL\n",
      "List of remaining words \n",
      " ['viruz', 'handuclean', 'feeeevvvveerrrr', 'facemaskk', 'sanitized/dry']\n",
      "List of correct words \n",
      " ['Microsoft', 'million']\n"
     ]
    }
   ],
   "source": [
    "# Named entity recognition(NER) is an important step that needs to be done before correcting a spelling\n",
    "# Organisations, cardinal, people, places shouldn't be corrected\n",
    "\n",
    "def NER(list_words,corrected):\n",
    "    for i in (list_words):\n",
    "\n",
    "        doc = nlp(i) \n",
    "        \n",
    "        for ent in doc.ents: \n",
    "            print(ent.text, ent.label_) \n",
    "            corrected.append(ent.text)\n",
    "            list_words.remove(ent.text)\n",
    "            \n",
    "    return [list_words,corrected]\n",
    "    \n",
    "            \n",
    "list_words,corrected=NER(list_words,corrected)\n",
    "print('List of remaining words \\n',list_words)\n",
    "print('List of correct words \\n',corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of remaining words \n",
      " ['handuclean', 'feeeevvvveerrrr', 'sanitized/dry']\n",
      "List of correct words \n",
      " ['Microsoft', 'million', 'virus', 'face-masks']\n"
     ]
    }
   ],
   "source": [
    "# Second function is to simply use the built in library of pyspellchecker\n",
    "# It has 2 built in function \n",
    "# spell.unknown which returns a set of all words which have a spelling mistake\n",
    "# spell.correction which tries to correct the given word based on the words present in the libraries vocabulary\n",
    "\n",
    "\n",
    "def preprocess_text(list_words,corrected):\n",
    "  index_array=[]\n",
    "  for i in ((list_words)):\n",
    "    if(len(list(spell.unknown([i])))!=0):           #Check for spelling error\n",
    "      index_array.append([i,spell.correction(i)])\n",
    "    else:\n",
    "        corrected.append(i)\n",
    "        list_words.remove(i)\n",
    "  \n",
    "  for i in ((index_array)):\n",
    "    if(len(list(spell.unknown([i[1]])))==0):\n",
    "      corrected.append(i[1])\n",
    "      list_words.remove(i[0])\n",
    "      #list_words[index_array[i][0]]=index_array[i][2]\n",
    "  return [list_words,corrected]\n",
    "\n",
    "\n",
    "list_words,corrected=preprocess_text(list_words,corrected)\n",
    "print('List of remaining words \\n',list_words)\n",
    "print('List of correct words \\n',corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['handuclean', 'fever', 'sanitized/dry']\n"
     ]
    }
   ],
   "source": [
    "#Function to get rid of recurring characters in a word, usually used when trying to imply a sentiment\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def recurring(list_words):\n",
    "  for q in range(len(list_words)):\n",
    "      chars=list(list_words[q])\n",
    "      col_count = Counter(chars)\n",
    "      pairs=list(col_count.items())\n",
    "      for i in range(0,len(pairs)):\n",
    "        if(pairs[i][1]>2):\n",
    "          pattern=pairs[i][0]+\"+\"\n",
    "          text_input=re.sub(pattern,pairs[i][0],text_input)\n",
    "        else:\n",
    "            text_input=list_words[q]\n",
    "      list_words[q]=text_input\n",
    "    \n",
    "  return list_words\n",
    "\n",
    "list_words=recurring(list_words)\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of remaining words \n",
      " ['handuclean', 'sanitized/dry']\n",
      "List of correct words \n",
      " ['Microsoft', 'million', 'virus', 'face-masks', 'fever']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_words,corrected=preprocess_text(list_words,corrected)\n",
    "print('List of remaining words \\n',list_words)\n",
    "print('List of correct words \\n',corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hand', 'clean', 'sanitized', 'dry']\n"
     ]
    }
   ],
   "source": [
    "#Function used to split words which don't are not seperated or have an error between two correcrtly spelt words\n",
    "\n",
    "def ninja(list_words):\n",
    "  array1=[]\n",
    "  for q in range(len(list_words)):\n",
    "      splits=wordninja.split(list_words[q])\n",
    "      for i in range(len(splits)):\n",
    "        if(len(splits[i])>2):\n",
    "          array1.append(splits[i])\n",
    "        \n",
    "  return array1\n",
    "\n",
    "list_words=ninja(list_words)\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform a final check on the words\n",
    "final_check=list(spell.unknown(list_words))\n",
    "for i in range(len(x)):\n",
    "    list_words.remove(x[i])\n",
    "corrected.extend(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Microsoft', 'million', 'virus', 'face-masks', 'fever', 'hand', 'clean', 'sanitized', 'dry']\n"
     ]
    }
   ],
   "source": [
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
